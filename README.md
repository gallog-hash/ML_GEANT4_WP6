
# ğŸ§  VAE-Based Super-Resolution for Geant4 Hadrontherapy

This repository implements a **Variational Autoencoder (VAE)** pipeline for
enhancing the resolution of simulation outputs from the **Geant4 Hadrontherapy
example**. It targets particle interaction data (e.g., Bragg peak patterns) and
reconstructs high-resolution outputs using generative modeling techniques.

---

## ğŸš€ Super-Resolution Task Overview

The **super-resolution task** involves:

- Using low-resolution simulation data from **Geant4 Hadrontherapy**
- Training a **Variational Autoencoder** to model latent representations
- Upsampling or reconstructing **high-resolution distributions**
- Optimizing model parameters via **Optuna**

---

## ğŸ§ª Key Features

- ğŸ”§ Modular training/optimization via `vae_training.py` and `vae_optimization.py`
- ğŸ¯ Study of various loss functions and network configurations
- ğŸ“ˆ Performance visualization and EDA (via Matplotlib and Optuna)
- ğŸ“¦ Utilities for dataset I/O, logging, and evaluation included in `utils/`

---

## ğŸ“ Directory Structure

```bash
src/
â”œâ”€â”€ vae_training.py                    # Main VAE training script
â”œâ”€â”€ vae_generate.py                    # Generate super-resolved outputs
â”œâ”€â”€ vae_optimization.py                # Hyperparameter optimization (Optuna)
â”œâ”€â”€ vae_optimization_analysis.py       # Analysis of optimization runs
â”œâ”€â”€ vae_post_analysis.py               # Post-training evaluation and visualization
â”œâ”€â”€ eda_main.py                        # Exploratory data analysis
â”œâ”€â”€ dataeng_main.py                    # Data engineering and preprocessing
â”œâ”€â”€ core/                              # Core VAE pipeline components
â”‚   â”œâ”€â”€ base_pipeline.py               # Base class for VAE operations
â”‚   â”œâ”€â”€ model_builder.py               # Factory for building VAE models
â”‚   â”œâ”€â”€ training_utils.py              # Training utilities and loss functions
â”‚   â”œâ”€â”€ models/                        # Custom PyTorch modules
â”‚   â”‚   â”œâ”€â”€ autoencoder.py             # AutoEncoder implementation
â”‚   â”‚   â””â”€â”€ activations.py             # Custom activations (PELU, ShiftedSoftplus)
â”‚   â””â”€â”€ preprocessing/                 # Data preprocessing pipeline
â”‚       â”œâ”€â”€ data_preprocessor.py       # VAEDataPreprocessor class
â”‚       â””â”€â”€ preprocessing_utils.py     # Preprocessing utilities
â”œâ”€â”€ utils/                             # Shared utilities
â”‚   â”œâ”€â”€ config_loader.py               # JSON configuration loading
â”‚   â”œâ”€â”€ data_loader_utils.py           # PyTorch DataLoader creation
â”‚   â”œâ”€â”€ filesystem_utils.py            # File I/O and directory management
â”‚   â”œâ”€â”€ logger.py                      # VAELogger for consistent logging
â”‚   â”œâ”€â”€ model_io.py                    # Model saving/loading
â”‚   â”œâ”€â”€ latent_utils.py                # Latent space analysis
â”‚   â””â”€â”€ plot_utils.py                  # Training metrics visualization
â””â”€â”€ configs/                           # Configuration files
    â”œâ”€â”€ trainer_config.json            # Main training configuration
    â”œâ”€â”€ generation_config.json         # Model generation settings
    â”œâ”€â”€ optuna_config.json             # Hyperparameter optimization
    â”œâ”€â”€ optimization_analysis_config.json # Optimization analysis settings
    â””â”€â”€ post_training_config.json      # Post-training evaluation
```

---

## ğŸ§° Usage

### 1. Train a VAE

```bash
python src/vae_training.py --config_path src/configs/trainer_config.json
```

### 2. Generate Super-Resolved Output

```bash
python src/vae_generate.py --config_path src/configs/generation_config.json
```

### 3. Optimize Hyperparameters

```bash
python src/vae_optimization.py --config_path src/configs/optuna_config.json
```

### 4. Analyze Optimization Results

```bash
python src/vae_optimization_analysis.py --config_path src/configs/optimization_analysis_config.json
```

### 5. Post-Training Analysis

```bash
python src/vae_post_analysis.py --config_path src/configs/post_training_config.json
```

### 6. Exploratory Data Analysis

```bash
python src/eda_main.py
```

### 7. Data Engineering and Preprocessing

```bash
python src/dataeng_main.py
```

### ğŸ“˜ Script Guides

- [vae_training.py](wiki/vae_training_guide.md) â€“ Training pipeline workflow and configuration
- [vae_generate.py](wiki/vae_generate_guide.md) â€“ Super-resolution generation from trained models
- [vae_optimization.py](wiki/vae_optimization_guide.md) â€“ Hyperparameter optimization with Optuna
- [vae_optimization_analysis.py](wiki/vae_optimization_analysis_guide.md) â€“ Analysis and visualization of optimization results
- [vae_post_analysis.py](wiki/vae_post_training_analysis_guide.md) â€“ Post-training evaluation and metrics

---

## ğŸ“Š Input Data

The VAE model is trained using Geant4 simulation output stored in the `Let.out`
file located in dataset directories such as:

```bash
data/thr96_1e8_v1um_cut1mm_ver_11-2-2/
â”œâ”€â”€ Let.out   â† used for training (LET profiles)
â”œâ”€â”€ Dose.out  â† used for visualization (dose profile overlay)
```

- **`Let.out`** contains voxelized **Linear Energy Transfer (LET)** values used
  directly as input data for training and generation.
- **`Dose.out`** contains the **energy deposition profile**, which is *not* used
  during training but is optionally overlaid in analysis plots for comparison
  with LET reconstructions.

The data typically represents high-resolution (âˆ¼1 Î¼m voxel size) 3D
distributions from **Geant4 Hadrontherapy** simulations. These are parsed into
3D arrays, normalized, and used to learn latent mappings for the
super-resolution task.

---

## ğŸ›  Dependencies

- Python â‰¥ 3.8
- PyTorch
- Optuna
- Numpy, Matplotlib, Seaborn
- [Geant4 simulation output](https://geant4.web.cern.ch/)

---

## ğŸ“„ License

This project is licensed under the terms of the **GNU General Public License v3.0**.  
See the [LICENSE](./LICENSE) file for full details.
